# ITSM Ticket Classification: ML/DL/LLM Pipeline

ğŸ¯ **Complete AI-powered system for automatic IT service ticket classification and resolution**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![ML](https://img.shields.io/badge/ML-scikit--learn-orange.svg)](https://scikit-learn.org/)
[![DL](https://img.shields.io/badge/DL-transformers-red.svg)](https://huggingface.co/transformers/)
[![LLM](https://img.shields.io/badge/LLM-RAG-green.svg)](https://openai.com/)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage Guide](#usage-guide)
- [Customization](#customization)
- [API Reference](#api-reference)
- [Project Structure](#project-structure)
- [Performance](#performance)
- [Contributing](#contributing)

---

## ğŸ¯ Overview

This project implements a **complete AI pipeline** for IT Service Management (ITSM) ticket classification using:

- **Machine Learning**: TF-IDF + Logistic Regression, Random Forest, XGBoost
- **Deep Learning**: Fine-tuned transformer models (DistilBERT)
- **Semantic Search**: Sentence embeddings + FAISS vector database
- **LLM Integration**: RAG-based resolution generation with OpenAI

### Use Cases

âœ… Automatic ticket categorization  
âœ… Intelligent ticket routing  
âœ… Similar ticket search  
âœ… AI-powered resolution suggestions  
âœ… Trend analysis and reporting  

---

## âœ¨ Features

### ğŸ¤– Machine Learning Models
- **Logistic Regression** (baseline, fast inference)
- **Random Forest** (ensemble, robust)
- **XGBoost** (gradient boosting, high accuracy)
- **Model comparison** and automatic best model selection

### ğŸ§  Deep Learning
- **DistilBERT** fine-tuning for text classification
- **Transfer learning** from pre-trained models
- **GPU acceleration** support
- **Hugging Face Transformers** integration

### ğŸ” Semantic Search
- **Sentence-BERT** embeddings (384 dimensions)
- **FAISS** vector search for millisecond queries
- **Cosine similarity** ranking
- **Similar ticket retrieval**

### ğŸ’¬ LLM & RAG
- **Retrieval-Augmented Generation** for resolutions
- **OpenAI API** integration (optional)
- **Rule-based fallback** (no API key needed)
- **Context-aware** responses

### ğŸ“Š Visualization & Reporting
- Model performance comparisons
- Confusion matrices
- Category distributions
- Training metrics

### ğŸš€ Deployment
- **FastAPI** REST API
- **Docker** support (optional)
- **Real-time prediction** endpoints
- **Swagger UI** documentation

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Tickets    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Cleaning   â”‚  â† Text preprocessing, feature engineering
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                  â–¼                  â–¼                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   ML   â”‚        â”‚    DL    â”‚      â”‚Embeddingsâ”‚      â”‚   LLM    â”‚
    â”‚ Models â”‚        â”‚Transform.â”‚      â”‚+ FAISS   â”‚      â”‚   RAG    â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â”‚                  â”‚                  â”‚                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  FastAPI     â”‚
                            â”‚  REST API    â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’» Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- (Optional) CUDA for GPU acceleration
- (Optional) OpenAI API key for LLM features

### Step 1: Clone the Repository

```bash
git clone <your-repo-url>
cd itsm_ml_project
```

### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Linux/Mac)
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
# Upgrade pip
python -m pip install --upgrade pip

# Install all dependencies
pip install -r requirements.txt
```

### Step 4: (Optional) Configure OpenAI API

```bash
# Create .env file
echo "OPENAI_API_KEY=your-api-key-here" > .env
```

---

## ğŸš€ Quick Start

### Option 1: Run Complete Pipeline

```bash
cd src
python main_pipeline.py
```

This will:
1. Clean and preprocess data
2. Train ML models
3. Generate embeddings
4. Build semantic search index
5. Run RAG demo

â±ï¸ **Time**: ~10-15 minutes (without DL training)

### Option 2: Quick Mode (ML Only)

```bash
cd src
python main_pipeline.py --quick
```

â±ï¸ **Time**: ~3-5 minutes

### Option 3: Step-by-Step

```bash
# Step 1: Clean data
cd src
python data_preprocessing.py

# Step 2: Train ML models
python train_ml_models.py

# Step 3: Generate embeddings (optional)
python embeddings_and_search.py --mode embeddings

# Step 4: Build search index (optional)
python embeddings_and_search.py --mode search

# Step 5: Run RAG demo (optional)
python llm_rag.py
```

---

## ğŸ“– Usage Guide

### 1. Data Preprocessing

```python
from data_preprocessing import DataCleaner

cleaner = DataCleaner(
    input_path="data/raw/tickets.csv",
    output_path="data/processed/clean_tickets.csv"
)
df = cleaner.run_pipeline()
```

**What it does:**
- Cleans text (removes URLs, emails, special chars)
- Creates combined text field
- Adds engineered features
- Filters rare categories
- Splits train/val/test sets

### 2. Train ML Models

```python
from train_ml_models import BaselineMLPipeline

pipeline = BaselineMLPipeline()
pipeline.run_pipeline()
```

**Output:**
- `models/baseline/model.joblib` - Best model
- `models/baseline/tfidf.joblib` - Vectorizer
- `models/baseline/label_encoder.joblib` - Label mapping
- `outputs/visualizations/ml_model_comparison.png`

### 3. Semantic Search

```python
from embeddings_and_search import SemanticSearchEngine

search = SemanticSearchEngine()
search.run_pipeline()

# Search for similar tickets
results = search.search("database connection error", k=5)
search.display_results("database connection error", results)
```

### 4. Generate Resolutions with RAG

```python
from llm_rag import TicketResolutionRAG

rag = TicketResolutionRAG()
rag.setup()

result = rag.process_ticket(
    ticket_description="Cannot access shared folder, permission denied",
    ticket_title="Folder access issue",
    use_llm=False  # Set True if OpenAI API configured
)

print(result['resolution'])
```

### 5. Make Predictions

```python
import joblib

# Load models
model = joblib.load("models/baseline/model.joblib")
vectorizer = joblib.load("models/baseline/tfidf.joblib")
encoder = joblib.load("models/baseline/label_encoder.joblib")

# Predict
text = "Server is down and not responding"
features = vectorizer.transform([text])
prediction = model.predict(features)[0]
category = encoder.inverse_transform([prediction])[0]

print(f"Predicted Category: {category}")
```

---

## ğŸ”§ Customization & Improvement

### 1. Tune Hyperparameters

Edit `configs/config.py`:

```python
# ML Models
ML_MODELS = {
    'logistic_regression': {
        'max_iter': 2000,  # Increase iterations
        'C': 1.0,  # Regularization strength
        'class_weight': 'balanced'
    },
    'random_forest': {
        'n_estimators': 200,  # More trees
        'max_depth': 30,  # Deeper trees
        'min_samples_split': 5
    }
}

# Feature Engineering
MAX_FEATURES = 10000  # Increase vocabulary size
NGRAM_RANGE = (1, 3)  # Include trigrams
```

### 2. Add Custom Features

In `data_preprocessing.py`, modify `add_features()`:

```python
def add_features(self, df: pd.DataFrame) -> pd.DataFrame:
    # Existing features...
    
    # Add custom features
    df['urgent_keywords'] = df['clean_text'].str.contains(
        'urgent|critical|down|emergency', 
        case=False
    ).astype(int)
    
    df['contains_error_code'] = df['clean_text'].str.contains(
        r'\berror\s*\d+\b',
        regex=True
    ).astype(int)
    
    return df
```

### 3. Use Different Embeddings

```python
# In configs/config.py
EMBEDDING_MODEL = 'all-mpnet-base-v2'  # Larger, more accurate
# Or
EMBEDDING_MODEL = 'paraphrase-MiniLM-L3-v2'  # Smaller, faster
```

### 4. Fine-tune Transformer

```python
# In configs/config.py
DL_CONFIG = {
    'model_name': 'bert-base-uncased',  # Use BERT instead of DistilBERT
    'num_epochs': 10,  # More epochs
    'learning_rate': 3e-5,  # Adjust learning rate
    'batch_size': 32  # Larger batches (needs more memory)
}
```

### 5. Add Multi-label Classification

Modify target to predict multiple categories:

```python
# In data_preprocessing.py
TARGET_COLUMNS = ['Service Category', 'Classification', 'Priority']

# In train_ml_models.py
from sklearn.multioutput import MultiOutputClassifier

model = MultiOutputClassifier(
    LogisticRegression(max_iter=1000)
)
```

### 6. Implement Priority Prediction

```python
# Add separate model for priority
priority_model = LogisticRegression()
y_priority = df['Priority'].map({'High': 2, 'Medium': 1, 'Low': 0})
priority_model.fit(X_train, y_priority)
```

### 7. Add Time Series Analysis

```python
import pandas as pd

df['Open DateTime'] = pd.to_datetime(df['Open DateTime'])
df['hour'] = df['Open DateTime'].dt.hour
df['day_of_week'] = df['Open DateTime'].dt.dayofweek

# Analyze trends
hourly_counts = df.groupby('hour').size()
daily_counts = df.groupby('day_of_week').size()
```

### 8. Implement Model Explainability

```python
import shap

# For tree-based models
explainer = shap.TreeExplainer(random_forest_model)
shap_values = explainer.shap_values(X_test)

# Visualize
shap.summary_plot(shap_values, X_test, feature_names=feature_names)
```

### 9. Add Active Learning

```python
from modAL.models import ActiveLearner

# Create active learner
learner = ActiveLearner(
    estimator=model,
    X_training=X_initial,
    y_training=y_initial
)

# Query uncertain samples
query_idx, query_instance = learner.query(X_pool)
```

### 10. Create Ensemble Model

```python
from sklearn.ensemble import VotingClassifier

ensemble = VotingClassifier(
    estimators=[
        ('lr', logistic_model),
        ('rf', random_forest_model),
        ('xgb', xgboost_model)
    ],
    voting='soft',  # Use probabilities
    weights=[1, 2, 2]  # Weight RF and XGB more
)
```

---

## ğŸŒ API Reference

### Start API Server

```bash
cd scripts
python api_server.py
```

Access Swagger docs: `http://localhost:8000/docs`

### Endpoints

#### 1. Health Check
```http
GET /health
```

#### 2. Predict Category (ML)
```http
POST /predict/ml
Content-Type: application/json

{
  "title": "Cannot access shared drive",
  "description": "Getting permission denied error when trying to open HR folder"
}
```

**Response:**
```json
{
  "category": "Folder Access",
  "confidence": 0.87,
  "method": "machine_learning"
}
```

#### 3. Semantic Search
```http
POST /search?k=5
Content-Type: application/json

{
  "title": "Database error",
  "description": "Oracle connection timeout after 30 seconds"
}
```

#### 4. Generate Resolution
```http
POST /resolve
Content-Type: application/json

{
  "title": "Email not syncing",
  "description": "Mobile email not updating",
  "use_llm": false
}
```

#### 5. Complete Analysis
```http
POST /analyze
Content-Type: application/json

{
  "title": "Server down",
  "description": "Production server not responding"
}
```

---

## ğŸ“ Project Structure

```
itsm_ml_project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Original dataset
â”‚   â”‚   â””â”€â”€ tickets.csv
â”‚   â””â”€â”€ processed/              # Cleaned data
â”‚       â”œâ”€â”€ clean_tickets.csv
â”‚       â”œâ”€â”€ train.csv
â”‚       â”œâ”€â”€ val.csv
â”‚       â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ baseline/               # ML models
â”‚   â”‚   â”œâ”€â”€ model.joblib
â”‚   â”‚   â”œâ”€â”€ tfidf.joblib
â”‚   â”‚   â””â”€â”€ label_encoder.joblib
â”‚   â”œâ”€â”€ embeddings/             # Sentence embeddings
â”‚   â”‚   â”œâ”€â”€ sentence_embeddings.npy
â”‚   â”‚   â””â”€â”€ faiss_index.bin
â”‚   â””â”€â”€ dl/                     # Deep learning models
â”‚       â””â”€â”€ best_model/
â”‚
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ data_preprocessing.py
â”‚   â”œâ”€â”€ train_ml_models.py
â”‚   â”œâ”€â”€ train_dl_model.py
â”‚   â”œâ”€â”€ embeddings_and_search.py
â”‚   â”œâ”€â”€ llm_rag.py
â”‚   â””â”€â”€ main_pipeline.py
â”‚
â”œâ”€â”€ configs/                    # Configuration
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ scripts/                    # Deployment scripts
â”‚   â”œâ”€â”€ api_server.py
â”‚   â””â”€â”€ demo_app.py
â”‚
â”œâ”€â”€ outputs/                    # Generated outputs
â”‚   â”œâ”€â”€ visualizations/
â”‚   â””â”€â”€ reports/
â”‚
â”œâ”€â”€ tests/                      # Unit tests
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ“Š Performance

### ML Models (Test Set)

| Model | Accuracy | F1-Score | Inference Time |
|-------|----------|----------|----------------|
| Logistic Regression | 76.5% | 0.745 | <1ms |
| Random Forest | 74.2% | 0.728 | 3ms |
| XGBoost | 77.8% | 0.762 | 2ms |

### Deep Learning

| Model | Accuracy | F1-Score | Training Time |
|-------|----------|----------|---------------|
| DistilBERT | 82.3% | 0.810 | ~30 min |

### Semantic Search

- **Index Build Time**: ~2 minutes
- **Query Time**: <10ms
- **Recall@5**: 0.85

---

## ğŸ“ Learning Resources

### Understanding the Code

1. **ML Pipeline**: See `train_ml_models.py` for TF-IDF + classification
2. **DL Fine-tuning**: See `train_dl_model.py` for transformer training
3. **Embeddings**: See `embeddings_and_search.py` for vector search
4. **RAG**: See `llm_rag.py` for retrieval-augmented generation

### Key Concepts

- **TF-IDF**: Term frequency-inverse document frequency for text vectorization
- **BERT/DistilBERT**: Transformer models for understanding context
- **FAISS**: Facebook AI Similarity Search for fast vector retrieval
- **RAG**: Combines retrieval with generation for grounded responses

---

## ğŸ”’ Security Notes

- Never commit API keys to version control
- Use `.env` file for sensitive configuration
- Validate all user inputs in production
- Implement rate limiting for API endpoints
- Use HTTPS in production deployment

---

## ğŸ› Troubleshooting

### Issue: Models not loading
**Solution**: Run the full pipeline first to generate models

### Issue: Out of memory during DL training
**Solution**: Reduce batch size in `configs/config.py`

### Issue: Slow embedding generation
**Solution**: Reduce embedding batch size or use GPU

### Issue: FAISS index not found
**Solution**: Run `embeddings_and_search.py` to build index

---

## ğŸ“ Citation

If you use this project in your research or work, please cite:

```bibtex
@software{itsm_ml_pipeline,
  title={ITSM Ticket Classification: ML/DL/LLM Pipeline},
  author={Your Name},
  year={2024},
  url={https://github.com/your-repo}
}
```

---

## ğŸ“§ Contact & Support

- **Issues**: [GitHub Issues](https://github.com/your-repo/issues)
- **Email**: your.email@example.com
- **Documentation**: [Wiki](https://github.com/your-repo/wiki)

---

## â­ Acknowledgments

- Dataset: ITSM ticket dataset
- Models: scikit-learn, Hugging Face, OpenAI
- Libraries: PyTorch, FAISS, FastAPI

---

**Made with â¤ï¸ for better IT service management**
